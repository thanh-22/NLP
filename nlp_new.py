# -*- coding: utf-8 -*-
"""NLP new

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T0lGdkK5WsCCSXPYeVB5XfOeLXDAPfkg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/datafinal.csv')
print("DataFrame loaded successfully. First 5 rows:")
print(df.head())

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize Tokenizer
num_words = 10000 # Example: max number of words to keep
tokenizer = Tokenizer(num_words=num_words, oov_token='<unk>')

# Ensure 'Description' column contains only strings and handle potential NaNs
df['Description'] = df['Description'].fillna('').astype(str)

# Fit tokenizer on 'Description' column
tokenizer.fit_on_texts(df['Description'])

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(df['Description'])

# Determine max sequence length (e.g., median or 95th percentile, for simplicity we'll use max length here)
max_sequence_length = max([len(x) for x in sequences])

# Pad sequences
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')

print(f"Original text sample: {df['Description'].iloc[0]}")
print(f"Tokenized sequence sample: {sequences[0]}")
print(f"Padded sequence sample: {padded_sequences[0]}")
print(f"Shape of padded_sequences: {padded_sequences.shape}")

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(df['label'])

print(f"Original labels: {df['label'].unique()}")
print(f"Encoded labels: {encoded_labels}")

# Split data into training and test sets (80/20 split)
X_train_full, X_test, y_train_full, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels)

# Split training data into training and validation sets (80/20 split of the training data, effectively 64/16/20 overall)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_val: {y_val.shape}")
print(f"Shape of y_test: {y_test.shape}")

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Permute, Multiply, Lambda, Activation
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

# Model parameters (from previous steps or chosen as examples)
embedding_dim = 128 # Example value for embedding dimension
lstm_units = 128 # Example value for LSTM units
dense_units = 64 # Example value for an intermediate Dense layer
num_classes = len(label_encoder.classes_) # Number of output classes

# --- Custom Attention Layer Definition (using functional API components) ---
def create_attention_layer(input_tensor):
    # input_tensor will be the output of the BiLSTM layer: (batch_size, timesteps, features)

    # Apply a Dense layer to the input_tensor to get 'attention scores'
    # Shape: (batch_size, timesteps, 1)
    attention_scores = Dense(1, activation='tanh', name='attention_scorer')(input_tensor)

    # Apply softmax activation to get attention weights
    # Softmax across the timesteps dimension (axis=1)
    attention_weights = Activation('softmax', name='attention_weights')(attention_scores)

    # Multiply the BiLSTM output by the attention weights (element-wise product)
    # Keras's Multiply layer handles broadcasting implicitly if dimensions align (e.g., (None, 256, 256) * (None, 256, 1))
    weighted_output = Multiply(name='weighted_output')([input_tensor, attention_weights])

    # Sum the weighted output over the timesteps dimension to get the context vector
    # Shape: (batch_size, features)
    context_vector = Lambda(lambda x: K.sum(x, axis=1), name='context_vector')(weighted_output)

    return context_vector

# --- Define the BiLSTM + Attention Model Architecture ---
# 1. Input layer
inputs = Input(shape=(max_sequence_length,), name='input_layer')

# 2. Embedding layer
x = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_sequence_length, name='embedding_layer')(inputs)

# 3. Bidirectional LSTM layer
x = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm_layer')(x)

# 4. Apply the custom Attention layer
x = create_attention_layer(x)

# 5. Add a Dense layer with 'relu' activation
x = Dense(dense_units, activation='relu', name='dense_relu_layer')(x)

# 6. Output Dense layer with 'softmax' activation for multi-class classification
outputs = Dense(num_classes, activation='softmax', name='output_layer')(x)

# Create the Model object
model_bilstm_attention = Model(inputs=inputs, outputs=outputs, name='BiLSTM_Attention_Model')

# --- Compile the model ---
model_bilstm_attention.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer-encoded labels
    metrics=['accuracy']
)

# --- Print model summary ---
print("\nBiLSTM + Attention Model Summary:")
model_bilstm_attention.summary()

print(f"Number of classes: {num_classes}")

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Permute, Multiply, Lambda, Activation
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

# Model parameters (from previous steps or chosen as examples)
embedding_dim = 128 # Example value for embedding dimension
lstm_units = 128 # Example value for LSTM units
dense_units = 64 # Example value for an intermediate Dense layer
num_classes = len(label_encoder.classes_) # Number of output classes

# --- Custom Attention Layer Definition (using functional API components) ---
def create_attention_layer(input_tensor):
    # input_tensor will be the output of the BiLSTM layer: (batch_size, timesteps, features)

    # Apply a Dense layer to the input_tensor to get 'attention scores'
    # Shape: (batch_size, timesteps, 1)
    attention_scores = Dense(1, activation='tanh', name='attention_scorer')(input_tensor)

    # Apply softmax activation to get attention weights
    # Softmax across the timesteps dimension (axis=1)
    attention_weights = Activation('softmax', name='attention_weights')(attention_scores)

    # Multiply the BiLSTM output by the attention weights (element-wise product)
    # Keras's Multiply layer handles broadcasting implicitly if dimensions align (e.g., (None, 256, 256) * (None, 256, 1))
    weighted_output = Multiply(name='weighted_output')([input_tensor, attention_weights])

    # Sum the weighted output over the timesteps dimension to get the context vector
    # Shape: (batch_size, features)
    context_vector = Lambda(lambda x: K.sum(x, axis=1), name='context_vector')(weighted_output)

    return context_vector

# --- Define the BiLSTM + Attention Model Architecture ---
# 1. Input layer
inputs = Input(shape=(max_sequence_length,), name='input_layer')

# 2. Embedding layer
x = Embedding(input_dim=num_words, output_dim=embedding_dim, name='embedding_layer')(inputs)

# 3. Bidirectional LSTM layer
x = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm_layer')(x)

# 4. Apply the custom Attention layer
x = create_attention_layer(x)

# 5. Add a Dense layer with 'relu' activation
x = Dense(dense_units, activation='relu', name='dense_relu_layer')(x)

# 6. Output Dense layer with 'softmax' activation for multi-class classification
outputs = Dense(num_classes, activation='softmax', name='output_layer')(x)

# Create the Model object
model_bilstm_attention = Model(inputs=inputs, outputs=outputs, name='BiLSTM_Attention_Model')

# --- Compile the model ---
model_bilstm_attention.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer-encoded labels
    metrics=['accuracy']
)

# --- Print model summary ---
print("\nBiLSTM + Attention Model Summary:")
model_bilstm_attention.summary()

print(f"Number of classes: {num_classes}")

epochs = 10
batch_size = 32

print("Starting training for BiLSTM + Attention model...")
history_bilstm_attention = model_bilstm_attention.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val)
)

print("BiLSTM + Attention model training complete.")

print("Evaluating BiLSTM + Attention model on test data...")
loss_bilstm_attention, accuracy_bilstm_attention = model_bilstm_attention.evaluate(X_test, y_test)

print(f"Test Loss: {loss_bilstm_attention:.4f}")
print(f"Test Accuracy: {accuracy_bilstm_attention:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Sử dụng mô hình BiLSTM + Attention đã huấn luyện để dự đoán nhãn cho tập kiểm tra (X_test).
print("Generating predictions for BiLSTM + Attention model...")
y_pred_proba_bilstm_attention = model_bilstm_attention.predict(X_test)
y_pred_bilstm_attention = np.argmax(y_pred_proba_bilstm_attention, axis=1)

# 2. Độ chính xác (accuracy) đã được tính trong bước đánh giá trước đó. In lại để tổng hợp.
print(f"\nTest Accuracy (reconfirmed): {accuracy_bilstm_attention:.4f}")

# 3. Tính toán các số liệu hiệu suất khác như Precision, Recall và F1-score cho toàn bộ dữ liệu kiểm tra, sử dụng cả 'macro' và 'weighted' average.
print("\nClassification Report (Macro and Weighted Averages) for BiLSTM + Attention Model:")
print(classification_report(y_test, y_pred_bilstm_attention, target_names=label_encoder.classes_, digits=4))

# 4. Tính toán Precision, Recall và F1-score cho từng lớp (per-class) riêng biệt.
# (This is already included in the classification_report above, but we can extract it explicitly if needed)

# 5. Tạo Ma trận nhầm lẫn (Confusion Matrix) và chuẩn hóa nó theo hàng (row-normalized).
cm = confusion_matrix(y_test, y_pred_bilstm_attention)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Normalized Confusion Matrix for BiLSTM + Attention Model')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# 6. Lưu mô hình BiLSTM + Attention đã huấn luyện vào một tệp.
model_save_path = 'bilstm_attention_model.keras'
model_bilstm_attention.save(model_save_path)
print(f"\nBiLSTM + Attention model saved to {model_save_path}")

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense
from tensorflow.keras.models import Model

# Model parameters (reusing from previous steps where applicable)
# num_words, max_sequence_length, embedding_dim, num_classes are already defined

filters = 128  # Example value for number of filters
kernel_size = 5 # Example value for convolution window size
dense_units_cnn = 64 # Example value for an intermediate Dense layer

# --- Define the CNN Model Architecture ---
# 1. Input layer
inputs_cnn = Input(shape=(max_sequence_length,), name='input_layer_cnn')

# 2. Embedding layer (reusing parameters from BiLSTM model)
x_cnn = Embedding(input_dim=num_words, output_dim=embedding_dim, name='embedding_layer_cnn')(inputs_cnn)

# 3. Convolutional 1D layer
x_cnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', name='conv1d_layer')(x_cnn)

# 4. Global MaxPooling 1D layer
x_cnn = GlobalMaxPooling1D(name='global_max_pooling_layer')(x_cnn)

# 5. Add a Dense layer with 'relu' activation
x_cnn = Dense(dense_units_cnn, activation='relu', name='dense_relu_layer_cnn')(x_cnn)

# 6. Output Dense layer with 'softmax' activation for multi-class classification
outputs_cnn = Dense(num_classes, activation='softmax', name='output_layer_cnn')(x_cnn)

# Create the Model object
model_cnn = Model(inputs=inputs_cnn, outputs=outputs_cnn, name='CNN_Model')

# --- Compile the model ---
model_cnn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer-encoded labels
    metrics=['accuracy']
)

# --- Print model summary ---
print("\nCNN Model Summary:")
model_cnn.summary()

print("Starting training for CNN model...")
history_cnn = model_cnn.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val)
)

print("CNN model training complete.")

print("Evaluating CNN model on test data...")
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)

print(f"Test Loss: {loss_cnn:.4f}")
print(f"Test Accuracy: {accuracy_cnn:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Sử dụng mô hình CNN đã huấn luyện để dự đoán nhãn cho tập kiểm tra (X_test).
print("Generating predictions for CNN model...")
y_pred_proba_cnn = model_cnn.predict(X_test)
y_pred_cnn = np.argmax(y_pred_proba_cnn, axis=1)

# 2. Độ chính xác (accuracy) đã được tính trong bước đánh giá trước đó. In lại để tổng hợp.
print(f"\nTest Accuracy (reconfirmed): {accuracy_cnn:.4f}")

# 3. Tính toán các số liệu hiệu suất khác như Precision, Recall và F1-score cho toàn bộ dữ liệu kiểm tra, sử dụng cả 'macro' và 'weighted' average.
print("\nClassification Report (Macro and Weighted Averages) for CNN Model:")
print(classification_report(y_test, y_pred_cnn, target_names=label_encoder.classes_, digits=4))

# 4. Tính toán Precision, Recall và F1-score cho từng lớp (per-class) riêng biệt.
# (This is already included in the classification_report above, but we can extract it explicitly if needed)

# 5. Tạo Ma trận nhầm lẫn (Confusion Matrix) và chuẩn hóa nó theo hàng (row-normalized).
cm_cnn = confusion_matrix(y_test, y_pred_cnn)
cm_normalized_cnn = cm_cnn.astype('float') / cm_cnn.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized_cnn, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Normalized Confusion Matrix for CNN Model')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# 6. Lưu mô hình CNN đã huấn luyện vào một tệp.
model_save_path_cnn = 'cnn_model.keras'
model_cnn.save(model_save_path_cnn)
print(f"\nCNN model saved to {model_save_path_cnn}")

import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense
from tensorflow.keras.models import Model

# Model parameters (reusing from previous steps where applicable)
# num_words, max_sequence_length, embedding_dim, num_classes are already defined
# lstm_units, filters, kernel_size, dense_units_cnn are already defined

# --- Define the RCNN Model Architecture ---
# A simplified RCNN variant: Embedding -> BiLSTM (contextual features) -> Conv1D (feature extraction) -> Pooling -> Dense

# 1. Input layer
inputs_rcnn = Input(shape=(max_sequence_length,), name='input_layer_rcnn')

# 2. Embedding layer
x_rcnn = Embedding(input_dim=num_words, output_dim=embedding_dim, name='embedding_layer_rcnn')(inputs_rcnn)

# 3. Bidirectional LSTM layer to get contextual representations
x_rcnn = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm_layer_rcnn')(x_rcnn)

# 4. Convolutional 1D layer applied to the contextual features
x_rcnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', name='conv1d_layer_rcnn')(x_rcnn)

# 5. Global MaxPooling 1D layer
x_rcnn = GlobalMaxPooling1D(name='global_max_pooling_layer_rcnn')(x_rcnn)

# 6. Add a Dense layer with 'relu' activation
x_rcnn = Dense(dense_units_cnn, activation='relu', name='dense_relu_layer_rcnn')(x_rcnn)

# 7. Output Dense layer with 'softmax' activation for multi-class classification
outputs_rcnn = Dense(num_classes, activation='softmax', name='output_layer_rcnn')(x_rcnn)

# Create the Model object
model_rcnn = Model(inputs=inputs_rcnn, outputs=outputs_rcnn, name='RCNN_Model')

# --- Compile the model ---
model_rcnn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy', # Use sparse_categorical_crossentropy for integer-encoded labels
    metrics=['accuracy']
)

# --- Print model summary ---
print("\nRCNN Model Summary:")
model_rcnn.summary()

print("Starting training for RCNN model...")
history_rcnn = model_rcnn.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val)
)

print("RCNN model training complete.")

print("Evaluating RCNN model on test data...")
loss_rcnn, accuracy_rcnn = model_rcnn.evaluate(X_test, y_test)

print(f"Test Loss: {loss_rcnn:.4f}")
print(f"Test Accuracy: {accuracy_rcnn:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Sử dụng mô hình RCNN đã huấn luyện để dự đoán nhãn cho tập kiểm tra (X_test).
print("Generating predictions for RCNN model...")
y_pred_proba_rcnn = model_rcnn.predict(X_test)
y_pred_rcnn = np.argmax(y_pred_proba_rcnn, axis=1)

# 2. Độ chính xác (accuracy) đã được tính trong bước đánh giá trước đó. In lại để tổng hợp.
print(f"\nTest Accuracy (reconfirmed): {accuracy_rcnn:.4f}")

# 3. Tính toán các số liệu hiệu suất khác như Precision, Recall và F1-score cho toàn bộ dữ liệu kiểm tra, sử dụng cả 'macro' và 'weighted' average.
print("\nClassification Report (Macro and Weighted Averages) for RCNN Model:")
print(classification_report(y_test, y_pred_rcnn, target_names=label_encoder.classes_, digits=4))

# 4. Tính toán Precision, Recall và F1-score cho từng lớp (per-class) riêng biệt.
# (This is already included in the classification_report above, but we can extract it explicitly if needed)

# 5. Tạo Ma trận nhầm lẫn (Confusion Matrix) và chuẩn hóa nó theo hàng (row-normalized).
cm_rcnn = confusion_matrix(y_test, y_pred_rcnn)
cm_normalized_rcnn = cm_rcnn.astype('float') / cm_rcnn.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized_rcnn, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Normalized Confusion Matrix for RCNN Model')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# 6. Lưu mô hình RCNN đã huấn luyện vào một tệp.
model_save_path_rcnn = 'rcnn_model.keras'
model_rcnn.save(model_save_path_rcnn)
print(f"\nRCNN model saved to {model_save_path_rcnn}")

import tensorflow as tf
from tensorflow.keras.layers import Layer, Input, Embedding, Dense, GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention, Dropout
from tensorflow.keras.models import Model

# Reusing parameters from previous steps or kernel state
# num_words, max_sequence_length, embedding_dim, num_classes are already defined
# dense_units is defined (64 for intermediate Dense layers)

# Transformer specific parameters (from kernel state)
num_heads = 2 # Number of attention heads
ff_dim = 32   # Hidden layer size in feed forward network inside TransformerBlock
dropout_rate = 0.1 # Dropout rate

# 1. Define TokenAndPositionEmbedding custom layer
class TokenAndPositionEmbedding(Layer):
    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):
        super(TokenAndPositionEmbedding, self).__init__(**kwargs)
        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.maxlen = maxlen
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        maxlen_current = tf.shape(inputs)[-1] # Use dynamic maxlen for positions
        positions = tf.range(start=0, limit=maxlen_current, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(inputs)
        return x + positions

    def get_config(self):
        config = super(TokenAndPositionEmbedding, self).get_config()
        config.update(
            {
                "maxlen": self.maxlen,
                "vocab_size": self.vocab_size,
                "embed_dim": self.embed_dim,
            }
        )
        return config

# 2. Define TransformerBlock custom layer
class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=dropout_rate, **kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [
                Dense(ff_dim, activation="relu"),
                Dense(embed_dim),
            ]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.ff_dim = ff_dim
        self.rate = rate

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
        config = super(TransformerBlock, self).get_config()
        config.update(
            {
                "embed_dim": self.embed_dim,
                "num_heads": self.num_heads,
                "ff_dim": self.ff_dim,
                "rate": self.rate,
            }
        )
        return config

# 3. Define the Transformer Encoder model architecture
inputs_transformer = Input(shape=(max_sequence_length,), name='input_layer_transformer')

# Token and Position Embedding layer
x_transformer = TokenAndPositionEmbedding(max_sequence_length, num_words, embedding_dim, name='token_and_position_embedding')(inputs_transformer)

# Transformer Block
x_transformer = TransformerBlock(embedding_dim, num_heads, ff_dim, name='transformer_block')(x_transformer)

# Global Average Pooling
x_transformer = GlobalAveragePooling1D(name='global_average_pooling')(x_transformer)

# Dropout layer
x_transformer = Dropout(dropout_rate, name='dropout_layer')(x_transformer)

# Dense layer with ReLU activation
x_transformer = Dense(dense_units, activation="relu", name='dense_relu_layer_transformer')(x_transformer)

# Output Dense layer with softmax activation
outputs_transformer = Dense(num_classes, activation="softmax", name='output_layer_transformer')(x_transformer)

# Create the Model object
model_transformer_encoder = Model(inputs=inputs_transformer, outputs=outputs_transformer, name='Transformer_Encoder_Model')

# Compile the model
model_transformer_encoder.compile(
    optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)

# Print model summary
print("\nTransformer Encoder Model Summary:")
model_transformer_encoder.summary()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# --- 1. Re-load Data --- (from cell 7357960d)
df = pd.read_csv('/content/drive/MyDrive/datafinal.csv')
print("DataFrame re-loaded.")

# --- 2. Re-tokenize and Pad Sequences --- (from cell 94e4146b)
num_words = 10000 # Example: max number of words to keep (re-using from global state)
tokenizer = Tokenizer(num_words=num_words, oov_token='<unk>')

# Fix: Ensure 'Description' column contains only strings and handle potential NaNs
df['Description'] = df['Description'].fillna('').astype(str)

tokenizer.fit_on_texts(df['Description'])
sequences = tokenizer.texts_to_sequences(df['Description'])

# Ensure max_sequence_length is defined; it was in global state but good to be explicit
# max_sequence_length is available from kernel state, value is 256
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')
print("Text re-tokenized and sequences re-padded.")

# --- 3. Re-encode Labels --- (from cell 267a759a)
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(df['label'])
print("Labels re-encoded.")

# --- 4. Re-split Data --- (from cell 267a759a)
X_train_full, X_test, y_train_full, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)
print("Data re-split into training, validation, and test sets.")

# --- 5. Re-define training hyperparameters --- (from cell 48544337)
epochs = 10
batch_size = 32
print(f"Training hyperparameters: epochs={epochs}, batch_size={batch_size}")

# --- 6. Train the Transformer Encoder model ---
print("Starting training for Transformer Encoder model...")
history_transformer_encoder = model_transformer_encoder.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val)
)

print("Transformer Encoder model training complete.")

print("Evaluating Transformer Encoder model on test data...")
loss_transformer_encoder, accuracy_transformer_encoder = model_transformer_encoder.evaluate(X_test, y_test)

print(f"Test Loss: {loss_transformer_encoder:.4f}")
print(f"Test Accuracy: {accuracy_transformer_encoder:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Sử dụng mô hình Transformer Encoder đã huấn luyện để dự đoán nhãn cho tập kiểm tra (X_test).
print("Generating predictions for Transformer Encoder model...")
y_pred_proba_transformer_encoder = model_transformer_encoder.predict(X_test)
y_pred_transformer_encoder = np.argmax(y_pred_proba_transformer_encoder, axis=1)

# 2. Độ chính xác (accuracy) đã được tính trong bước đánh giá trước đó. In lại để tổng hợp.
print(f"\nTest Accuracy (reconfirmed): {accuracy_transformer_encoder:.4f}")

# 3. Tính toán các số liệu hiệu suất khác như Precision, Recall và F1-score cho toàn bộ dữ liệu kiểm tra, sử dụng cả 'macro' và 'weighted' average.
print("\nClassification Report (Macro and Weighted Averages) for Transformer Encoder Model:")
print(classification_report(y_test, y_pred_transformer_encoder, target_names=label_encoder.classes_, digits=4))

# 4. Tính toán Precision, Recall và F1-score cho từng lớp (per-class) riêng biệt.
# (This is already included in the classification_report above, but we can extract it explicitly if needed)

# 5. Tạo Ma trận nhầm lẫn (Confusion Matrix) và chuẩn hóa nó theo hàng (row-normalized).
cm_transformer_encoder = confusion_matrix(y_test, y_pred_transformer_encoder)
cm_normalized_transformer_encoder = cm_transformer_encoder.astype('float') / cm_transformer_encoder.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized_transformer_encoder, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Normalized Confusion Matrix for Transformer Encoder Model')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# 6. Lưu mô hình Transformer Encoder đã huấn luyện vào một tệp.
model_save_path_transformer_encoder = 'transformer_encoder_model.keras'
model_transformer_encoder.save(model_save_path_transformer_encoder)
print(f"\nTransformer Encoder model saved to {model_save_path_transformer_encoder}")

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, BertTokenizerFast, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("/content/drive/MyDrive/datafinal.csv")

label2id = {"CRITICAL":0, "MAJOR":1, "MINOR":2, "UNCLEAR":3}
id2label = {v:k for k,v in label2id.items()}

df["label_id"] = df["label"].map(label2id)

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label_id"], random_state=42)
train_df, val_df  = train_test_split(train_df, test_size=0.1111, stratify=train_df["label_id"], random_state=42)

from transformers import BertTokenizerFast, BertModel
import torch
import numpy as np
import tensorflow as tf # Added tensorflow import

def clean_texts(texts):
    return [str(x) if isinstance(x, str) else "" for x in texts]
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def encode(texts, max_len=128):
    texts = clean_texts(texts)
    return tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=max_len,
        return_tensors="pt"
    )

X_train = encode(train_df["Description"].tolist())
X_val   = encode(val_df["Description"].tolist())
X_test  = encode(test_df["Description"].tolist())

y_train = tf.keras.utils.to_categorical(train_df["label_id"], num_classes=4)
y_val   = tf.keras.utils.to_categorical(val_df["label_id"], num_classes=4)
y_test  = tf.keras.utils.to_categorical(test_df["label_id"], num_classes=4)

class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.enc = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.enc.items()}
        item["labels"] = self.labels[idx]
        return item

train_ds = TextDataset(X_train, y_train)
val_ds   = TextDataset(X_val, y_val)
test_ds  = TextDataset(X_test, y_test)

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=16)
test_loader  = DataLoader(test_ds, batch_size=16)

class AttentivePooling(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size, 1)

    def forward(self, x, mask):
        scores = self.attn(x).squeeze(-1)
        scores = scores.masked_fill(mask == 0, -1e9)
        weights = torch.softmax(scores, dim=1).unsqueeze(-1)
        return torch.sum(weights * x, dim=1)

class BertAttentiveClassifier(nn.Module):
    def __init__(self, num_classes=4, dropout=0.3, ms_k=5):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.pool = AttentivePooling(self.bert.config.hidden_size)
        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(ms_k)])
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = self.pool(out, attention_mask)
        logits = torch.stack([self.fc(d(pooled)) for d in self.dropouts]).mean(0)
        return logits
def set_bert_trainable(model, n_unfreeze):
    for p in model.bert.parameters():
        p.requires_grad = False
    if n_unfreeze > 0:
        for layer in model.bert.encoder.layer[-n_unfreeze:]:
            for p in layer.parameters():
                p.requires_grad = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertAttentiveClassifier().to(device)

criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

optimizer = torch.optim.AdamW([
    {"params": model.bert.parameters(), "lr": 2e-5},
    {"params": model.fc.parameters(), "lr": 1e-3},
    {"params": model.pool.parameters(), "lr": 1e-3},
], weight_decay=1e-4)

def train_epoch(model, loader, accum_steps=4):
    model.train()
    total_loss = 0
    optimizer.zero_grad()

    for i, batch in enumerate(tqdm(loader)):
        input_ids = batch["input_ids"].to(device)
        mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, mask)
        loss = criterion(logits, labels) / accum_steps
        loss.backward()

        if (i + 1) % accum_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        total_loss += loss.item() * accum_steps

    return total_loss / len(loader)
def eval_epoch(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids, mask)
            preds.append(logits.argmax(1).cpu().numpy())
            trues.append(labels.cpu().numpy())
    return np.concatenate(preds), np.concatenate(trues)

# Stage 1: freeze BERT
set_bert_trainable(model, 0)
for _ in range(3):
    print("Loss:", train_epoch(model, train_loader))

# Stage 2: unfreeze top 2 layers
set_bert_trainable(model, 2)
for _ in range(3):
    print("Loss:", train_epoch(model, train_loader))

# Stage 3: unfreeze all
set_bert_trainable(model, 12)
for _ in range(4):
    print("Loss:", train_epoch(model, train_loader))

from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np # Ensure numpy is imported

# Use the eval_epoch function to get predictions and true labels
# eval_epoch returns predicted class indices and true labels (which are one-hot encoded if y_test was one-hot)
predicted_labels_cls, true_labels_one_hot = eval_epoch(model, test_loader)

# Convert true one-hot labels to class indices
y_true_cls = np.argmax(true_labels_one_hot, axis=1)
y_pred_cls = predicted_labels_cls # eval_epoch already returns class indices for predictions

print("Accuracy:", accuracy_score(y_true_cls, y_pred_cls))
print("Macro-F1:", f1_score(y_true_cls, y_pred_cls, average="macro"))
print("Weighted-F1:", f1_score(y_true_cls, y_pred_cls, average="weighted"))
print(classification_report(y_true_cls, y_pred_cls, target_names=list(label2id.keys())))

import torch

model_save_path_bert = 'bert_custom_classifier.pth'
torch.save(model.state_dict(), model_save_path_bert)
print(f"BERT + Custom Head model saved to {model_save_path_bert}")

import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_true_cls, y_pred_cls, normalize="true")

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt=".2f", xticklabels=label2id.keys(), yticklabels=label2id.keys(), cmap="Blues")
plt.title("Row-normalized Confusion Matrix")
plt.ylabel("True")
plt.xlabel("Pred")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("/content/drive/MyDrive/datafinal.csv")
df = df[["Description", "label"]].dropna()

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
train_df, val_df  = train_test_split(train_df, test_size=0.1, stratify=train_df["label"], random_state=42)

import torch
import torch.nn as nn
from transformers import BertModel

class AttentivePooling(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size, 1)

    def forward(self, x, mask):
        scores = self.attn(x).squeeze(-1)     # [B, T]
        scores = scores.masked_fill(mask == 0, -1e9)
        weights = torch.softmax(scores, dim=1).unsqueeze(-1)
        pooled = torch.sum(weights * x, dim=1)
        return pooled


class BertFullClassifier(nn.Module):
    def __init__(self, num_classes=4, ms_dropout_k=5, p=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.pool = AttentivePooling(self.bert.config.hidden_size)
        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(ms_dropout_k)])
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = self.pool(out, attention_mask)
        logits = torch.stack([self.fc(d(pooled)) for d in self.dropouts], dim=0).mean(dim=0)
        return logits


class BertNoAttnPooling(nn.Module):
    """❌ w/o Attentive Pooling: CLS only"""
    def __init__(self, num_classes=4, ms_dropout_k=5, p=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(ms_dropout_k)])
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        pooled = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        logits = torch.stack([self.fc(d(pooled)) for d in self.dropouts], dim=0).mean(dim=0)
        return logits


class BertNoMSDropout(nn.Module):
    """❌ w/o Multi-Sample Dropout: single dropout"""
    def __init__(self, num_classes=4, p=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.pool = AttentivePooling(self.bert.config.hidden_size)
        self.dropout = nn.Dropout(p)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = self.pool(out, attention_mask)
        logits = self.fc(self.dropout(pooled))
        return logits

from sklearn.metrics import classification_report, accuracy_score, f1_score

def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(dataloader)


def evaluate(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids, mask)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    macro_f1 = f1_score(all_labels, all_preds, average="macro")
    weighted_f1 = f1_score(all_labels, all_preds, average="weighted")

    print("Accuracy:", acc)
    print("Macro-F1:", macro_f1)
    print("Weighted-F1:", weighted_f1)
    print(classification_report(all_labels, all_preds, digits=4))

    return acc, macro_f1, weighted_f1

from sklearn.metrics import classification_report, accuracy_score, f1_score

def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(dataloader)


def evaluate(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids, mask)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    macro_f1 = f1_score(all_labels, all_preds, average="macro")
    weighted_f1 = f1_score(all_labels, all_preds, average="weighted")

    print("Accuracy:", acc)
    print("Macro-F1:", macro_f1)
    print("Weighted-F1:", weighted_f1)
    print(classification_report(all_labels, all_preds, digits=4))

    return acc, macro_f1, weighted_f1

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, BertTokenizerFast, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from tqdm import tqdm


# --- Class Definitions ---
class AttentivePooling(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size, 1)

    def forward(self, x, mask):
        scores = self.attn(x).squeeze(-1)
        scores = scores.masked_fill(mask == 0, -1e9)
        weights = torch.softmax(scores, dim=1).unsqueeze(-1)
        return torch.sum(weights * x, dim=1)

class BertAttentiveClassifier(nn.Module):
    def __init__(self, num_classes=4, dropout=0.3, ms_k=5):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.pool = AttentivePooling(self.bert.config.hidden_size)
        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(ms_k)])
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = self.pool(out, attention_mask)
        logits = torch.stack([self.fc(d(pooled)) for d in self.dropouts]).mean(0)
        return logits

def set_bert_trainable(model, n_unfreeze):
    for p in model.bert.parameters():
        p.requires_grad = False
    if n_unfreeze > 0:
        for layer in model.bert.encoder.layer[-n_unfreeze:]:
            for p in layer.parameters():
                p.requires_grad = True

class BertNoAttnPooling(nn.Module):
    """❌ w/o Attentive Pooling: CLS only"""
    def __init__(self, num_classes=4, ms_dropout_k=5, p=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(ms_dropout_k)])
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        pooled = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        logits = torch.stack([self.fc(d(pooled)) for d in self.dropouts], dim=0).mean(dim=0)
        return logits

class BertNoMSDropout(nn.Module):
    """❌ w/o Multi-Sample Dropout: single dropout"""
    def __init__(self, num_classes=4, p=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.pool = AttentivePooling(self.bert.config.hidden_size)
        self.dropout = nn.Dropout(p)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = self.pool(out, attention_mask)
        logits = self.fc(self.dropout(pooled))
        return logits

# --- Training and Evaluation Functions ---
def train_epoch(model, loader, optimizer, criterion, device, accum_steps=4):
    model.train()
    total_loss = 0
    optimizer.zero_grad()

    for i, batch in enumerate(tqdm(loader)):
        input_ids = batch["input_ids"].to(device)
        mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, mask)
        loss = criterion(logits, labels) / accum_steps
        loss.backward()

        if (i + 1) % accum_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        total_loss += loss.item() * accum_steps

    return total_loss / len(loader)

def eval_epoch(model, loader, device):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids, mask)
            preds.append(logits.argmax(1).cpu().numpy())
            trues.append(labels.cpu().numpy())
    return np.concatenate(preds), np.concatenate(trues)

# --- Experiment Orchestration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def run_experiment(model_class, n_epochs_stage1=3, n_epochs_stage2=3, n_epochs_stage3=4):
    model = model_class().to(device)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = torch.optim.AdamW([
        {"params": model.bert.parameters(), "lr": 2e-5},
        {"params": model.fc.parameters(), "lr": 1e-3},
        {"params": model.pool.parameters(), "lr": 1e-3 if hasattr(model, 'pool') else 0},
    ], weight_decay=1e-4)

    # Stage 1: freeze BERT
    set_bert_trainable(model, 0)
    print("\nStage 1: Freezing BERT, training custom head")
    for epoch in range(n_epochs_stage1):
        print(f"Epoch {epoch+1}/{n_epochs_stage1} Loss: {train_epoch(model, train_loader, optimizer, criterion, device):.4f}")

    # Stage 2: unfreeze top 2 layers
    set_bert_trainable(model, 2)
    print("\nStage 2: Unfreezing top 2 BERT layers")
    for epoch in range(n_epochs_stage2):
        print(f"Epoch {epoch+1}/{n_epochs_stage2} Loss: {train_epoch(model, train_loader, optimizer, criterion, device):.4f}")

    # Stage 3: unfreeze all
    set_bert_trainable(model, 12)
    print("\nStage 3: Unfreezing all BERT layers")
    for epoch in range(n_epochs_stage3):
        print(f"Epoch {epoch+1}/{n_epochs_stage3} Loss: {train_epoch(model, train_loader, optimizer, criterion, device):.4f}")

    # Evaluate the trained model
    print("\nEvaluating on test data...")
    predicted_labels_cls, true_labels_one_hot = eval_epoch(model, test_loader, device) # Use the global test_loader
    y_true_cls = np.argmax(true_labels_one_hot, axis=1)
    y_pred_cls = predicted_labels_cls
    accuracy = accuracy_score(y_true_cls, y_pred_cls)

    return accuracy

# --- Run Experiments ---
print("\nFULL MODEL")
full_accuracy = run_experiment(BertAttentiveClassifier)
print(f"Full model accuracy: {full_accuracy:.4f}")

print("\n❌ NO ATTENTIVE POOLING")
no_attn_accuracy = run_experiment(BertNoAttnPooling)
print(f"No Attentive Pooling accuracy: {no_attn_accuracy:.4f}")

print("\n❌ NO MULTI-SAMPLE DROPOUT")
no_ms_accuracy = run_experiment(BertNoMSDropout)
print(f"No Multi-Sample Dropout accuracy: {no_ms_accuracy:.4f}")

print("\n==== ABLATION SUMMARY ====")
print(f"Full model accuracy: {full_accuracy:.4f}")
print(f"No Attentive Pooling accuracy: {no_attn_accuracy:.4f}")
print(f"No Multi-Sample Dropout accuracy: {no_ms_accuracy:.4f}")

import pandas as pd

new_df = pd.read_csv("/content/drive/MyDrive/datafinal.csv")
new_texts = new_df["Description"].astype(str).tolist()

from transformers import BertTokenizerFast
import torch
from torch.utils.data import DataLoader, TensorDataset

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def encode_texts(texts, max_len=128):
    enc = tokenizer(
        list(texts.astype(str)),
        truncation=True,
        padding=True,
        max_length=max_len,
        return_tensors="pt"
    )
    return enc["input_ids"], enc["attention_mask"]

input_ids, attention_mask = encode_texts(new_df["Description"])

# Giả sử bạn đã lưu model
model = BertAttentiveClassifier(num_classes=4)
model.load_state_dict(torch.load("bert_custom_classifier (1).pth", map_location=device))
model.to(device)
model.eval()

from torch.utils.data import DataLoader, TensorDataset

test_dataset = TensorDataset(input_ids, attention_mask)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

model.eval()
all_preds = []
all_probs = []

with torch.no_grad():
    for batch in test_loader:
        input_ids_batch = batch[0].to(device)
        attention_mask_batch = batch[1].to(device)

        logits = model(input_ids_batch, attention_mask_batch)
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_probs.extend(probs.max(dim=1).values.cpu().numpy())
id2label = {
    0: "CRITICAL",
    1: "MAJOR",
    2: "MINOR",
    3: "UNCLEAR"
}

new_df["pred_label"] = [id2label[i] for i in all_preds]
new_df.to_csv("/content/drive/MyDrive/new_test_predictions.csv", index=False)

new_df.head()

errors = new_df[new_df["label"] != new_df["pred_label"]]

print("Tổng số lỗi:", len(errors))

# Lấy ngẫu nhiên 30 lỗi
errors_30 = errors.sample(n=min(50, len(errors)), random_state=42)

errors_30[["Description", "label", "pred_label", ]].to_csv(
    "/content/drive/MyDrive/30_error_cases.csv", index=False
)

errors_30.head(10)

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each label
label_counts = new_df['label'].value_counts().reset_index()
label_counts.columns = ['label', 'count']

# Create the bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='label', y='count', hue='label', data=label_counts, palette='viridis', legend=False)
plt.title('Distribution of Labels in datafinal.csv')
plt.xlabel('Label')
plt.ylabel('Number of Samples')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()